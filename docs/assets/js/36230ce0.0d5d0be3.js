"use strict";(self.webpackChunkrag_eval_docs=self.webpackChunkrag_eval_docs||[]).push([[305],{5258:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>i,contentTitle:()=>l,default:()=>d,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"evaluator/evaluator","title":"Evaluator","description":"Overview","source":"@site/docs/evaluator/evaluator.md","sourceDirName":"evaluator","slug":"/evaluator/","permalink":"/docs/evaluator/","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/evaluator/evaluator.md","tags":[],"version":"current","frontMatter":{"id":"evaluator","title":"Evaluator"},"sidebar":"docs","previous":{"title":"Cumulative NDCG","permalink":"/docs/metrics/reranker/cumulative-ndcg"},"next":{"title":"Test Dataset Generation","permalink":"/docs/test-dataset-generation/"}}');var a=t(4848),s=t(8453);const o={id:"evaluator",title:"Evaluator"},l="Evaluator",i={},c=[{value:"<strong>Overview</strong>",id:"overview",level:2},{value:"Steps to evaluate your pipeline",id:"steps-to-evaluate-your-pipeline",level:3},{value:"Lower-level metric usage",id:"lower-level-metric-usage",level:4}];function u(e){const r={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(r.header,{children:(0,a.jsx)(r.h1,{id:"evaluator",children:"Evaluator"})}),"\n",(0,a.jsx)(r.h2,{id:"overview",children:(0,a.jsx)(r.strong,{children:"Overview"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsx)(r.li,{children:"The Evaluator is a convenience wrapper to run multiple metrics over model outputs and retrieval results."}),"\n",(0,a.jsx)(r.li,{children:"It orchestrates generation evaluation (text-generation metrics), retrieval evaluation (precision/recall/sufficiency), and reranker evaluation (NDCG, MAP, MRR). It produces CSV summaries by default."}),"\n"]}),"\n",(0,a.jsxs)(r.blockquote,{children:["\n",(0,a.jsx)(r.p,{children:"Quick notes"}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsx)(r.li,{children:"The Evaluator uses project metric classes (e.g., BartScore, BertScore, RougeScore, SemScore, PrecisionScore, RecallScore, MeanAP, MeanRR, RerankerNDCG, CumulativeNDCG, etc.). These metrics are in vero.metrics and are referenced internally."}),"\n",(0,a.jsx)(r.li,{children:'Many methods expect particular CSV column names (see "Expected CSV schemas").'}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(r.h3,{id:"steps-to-evaluate-your-pipeline",children:"Steps to evaluate your pipeline"}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Step 1 - Generation evaluation"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsx)(r.li,{children:'Input: a CSV with "Context Retrieved" and "Answer" columns.'}),"\n",(0,a.jsx)(r.li,{children:"Result: Generation_Scores.csv with columns such as SemScore, BertScore, RougeLScore, BARTScore, BLUERTScore, G-Eval (Faithfulness)."}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:"Example:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-py",children:'from vero.evaluator.evaluator import Evaluator\n\nevaluator = Evaluator()\n# data_path must point to a CSV with columns "Context Retrieved" and "Answer"\ndf_scores = evaluator.evaluate_generation(data_path=\'testing.csv\')\nprint(df_scores.head())\n'})}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Step 2 - Preparing reranker inputs (parse ground truth + retriever output)"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsx)(r.li,{children:"Use parse_retriever_data to convert ground-truth chunk ids and retriever outputs into a ranked_chunks_data.csv suitable for reranker evaluation."}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:"Example:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-py",children:"from vero.evaluator.evaluator import Evaluator\n\nevaluator = Evaluator()\n# ground_truth_path: dataset with 'Chunk IDs' and 'Less Relevant Chunk IDs' columns\n# data_path: retriever output with 'Context Retrieved' containing \"id='...'\"\nevaluator.parse_retriever_data(\n    ground_truth_path='test_dataset_generator.csv',\n    data_path='testing.csv'\n)\n# This will produce 'ranked_chunks_data.csv'\n"})}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Step 3 - Retrieval evaluation (precision, recall, sufficiency)"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:["Inputs:","\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsx)(r.li,{children:"retriever_data_path: a CSV that contains 'Retrieved Chunk IDs' and 'True Chunk IDs' columns (lists or strings)."}),"\n",(0,a.jsx)(r.li,{children:"data_path: the generation CSV with 'Context Retrieved' and 'Question' (for sufficiency)."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(r.li,{children:"Result: Retrieval_Scores.csv"}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:"Example:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-py",children:"from vero.evaluator.evaluator import Evaluator\n\nevaluator = Evaluator()\ndf_retrieval_scores = evaluator.evaluate_retrieval(\n    data_path='testing.csv',\n    retriever_data_path='ranked_chunks_data.csv'\n)\nprint(df_retrieval_scores.head())\n"})}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Step 4 - Reranker evaluation (MAP, MRR, NDCG)"}),"\nExample:"]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-py",children:"from vero.evaluator.evaluator import Evaluator\n\nevaluator = Evaluator()\ndf_reranker_scores = evaluator.evaluate_reranker(\n    ground_truth_path='test_dataset_generator.csv',\n    retriever_data_path='ranked_chunks_data.csv'\n)\nprint(df_reranker_scores)\n"})}),"\n",(0,a.jsx)(r.h4,{id:"lower-level-metric-usage",children:"Lower-level metric usage"}),"\n",(0,a.jsx)(r.p,{children:"To run a single metric directly you can instantiate the metric class. For example, to compute BARTScore or BertScore per pair:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-py",children:"from vero.metrics import BartScore, BertScore\n\nwith BartScore() as bs:\n    bart_results = [bs.evaluate(context, answer) for context, answer in zip(contexts, answers)]\n\nwith BertScore() as bert:\n    bert_results = [bert.evaluate(context, answer) for context, answer in zip(contexts, answers)]\n"})})]})}function d(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,a.jsx)(r,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},8453:(e,r,t)=>{t.d(r,{R:()=>o,x:()=>l});var n=t(6540);const a={},s=n.createContext(a);function o(e){const r=n.useContext(s);return n.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function l(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),n.createElement(s.Provider,{value:r},e.children)}}}]);