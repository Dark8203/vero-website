"use strict";(self.webpackChunkrag_eval_docs=self.webpackChunkrag_eval_docs||[]).push([[973],{3034:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>d,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"test-dataset-generation/test-dataset-generation","title":"Test Dataset Generation","description":"Overview","source":"@site/docs/test-dataset-generation/test-dataset-generation.md","sourceDirName":"test-dataset-generation","slug":"/test-dataset-generation/","permalink":"/docs/test-dataset-generation/","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/test-dataset-generation/test-dataset-generation.md","tags":[],"version":"current","frontMatter":{"id":"test-dataset-generation","title":"Test Dataset Generation"},"sidebar":"docs","previous":{"title":"Evaluator","permalink":"/docs/evaluator/"},"next":{"title":"Report Generation","permalink":"/docs/report-generation/"}}');var r=n(4848),a=n(8453);const i={id:"test-dataset-generation",title:"Test Dataset Generation"},d="Test Dataset Generation",o={},l=[{value:"<strong>Overview</strong>",id:"overview",level:2},{value:"When to use",id:"when-to-use",level:3},{value:"Core function",id:"core-function",level:3},{value:"Parameters",id:"parameters",level:3},{value:"Return",id:"return",level:3},{value:"<strong>Example</strong>",id:"example",level:3},{value:"Practical tips",id:"practical-tips",level:3}];function c(e){const t={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"test-dataset-generation",children:"Test Dataset Generation"})}),"\n",(0,r.jsx)(t.h2,{id:"overview",children:(0,r.jsx)(t.strong,{children:"Overview"})}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"The Test Dataset Generation module creates high-quality question-answer pairs derived from your document collection. It generates challenging queries designed to reveal retrieval and reasoning failures in RAG systems (e.g., boundary synthesis, chunk-length bias, intent understanding)."}),"\n",(0,r.jsx)(t.li,{children:"Internally it chunks documents, clusters related chunks, and uses an LLM to produce QA items with ground-truth chunk IDs and metadata."}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"when-to-use",children:"When to use"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"To create evaluation sets for retriever, reranker, and generation metrics."}),"\n",(0,r.jsx)(t.li,{children:"To stress-test retrieval behavior (chunk-length bias, boundary content) and query understanding."}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"core-function",children:"Core function"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.code,{children:"generate_and_save(data_path, usecase, save_path_dir='test_dataset', n_queries=100)"})}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"parameters",children:"Parameters"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"data_path (str)"}),": Path to a directory containing source PDFs. The generator uses a PDF loader to read documents."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"usecase (str)"})," : Description of the dataset/use-case."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"save_path_dir (str)"}),": Output directory path (default: 'test_dataset')."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"n_queries (int)"}),": Number of QA queries to generate (min 100). The generator groups queries across several prompt styles."]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"return",children:"Return"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["Writes test dataset generated and relevant files to ",(0,r.jsx)(t.code,{children:"save_path_dir"})," and returns a short success message."]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"example",children:(0,r.jsx)(t.strong,{children:"Example"})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-py",children:"from vero.test_dataset_generator import generate_and_save\n\n# Generate 40 queries from PDFs stored in ./data/pdfs and save as test_dataset.csv\ngenerate_and_save(\n    data_path='./data/pdfs',\n    usecase='Vitamin chatbot catering to general users for their daily queries',\n    save_path_dir='test_dataset',\n    n_queries=100\n)\n"})}),"\n",(0,r.jsxs)(t.blockquote,{children:["\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"Using the dataset with the Evaluator"})}),"\n",(0,r.jsxs)(t.p,{children:["The Evaluator expects a ground truth CSV with chunk IDs when running reranker / retrieval evaluations. The produced CSV is directly usable as ",(0,r.jsx)(t.code,{children:"ground_truth_path"})," in ",(0,r.jsx)(t.code,{children:"Evaluator.parse_retriever_data"})," or ",(0,r.jsx)(t.code,{children:"evaluate_reranker"}),"."]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"practical-tips",children:"Practical tips"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["Recommended ",(0,r.jsx)(t.code,{children:"n_queries"}),": 100\u2013500 depending on evaluation budget."]}),"\n",(0,r.jsx)(t.li,{children:"Reproducibility: You can re-run with the same document set; consider saving intermediate chunking outputs if you need stable chunk IDs across runs."}),"\n"]})]})}function u(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>d});var s=n(6540);const r={},a=s.createContext(r);function i(e){const t=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(a.Provider,{value:t},e.children)}}}]);