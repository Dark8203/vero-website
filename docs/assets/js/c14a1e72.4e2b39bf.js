"use strict";(self.webpackChunkrag_eval_docs=self.webpackChunkrag_eval_docs||[]).push([[131],{8453:(e,r,t)=>{t.d(r,{R:()=>l,x:()=>o});var n=t(6540);const s={},c=n.createContext(s);function l(e){const r=n.useContext(c);return n.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),n.createElement(c.Provider,{value:r},e.children)}},9990:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>i,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>n,toc:()=>a});const n=JSON.parse('{"id":"metrics/retriever/recallscore","title":"Recall Score","description":"Measures how many ground truth items are retrieved.","source":"@site/docs/metrics/retriever/recall.md","sourceDirName":"metrics/retriever","slug":"/metrics/retriever/recallscore","permalink":"/docs/metrics/retriever/recallscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/retriever/recall.md","tags":[],"version":"current","frontMatter":{"id":"recallscore","title":"Recall Score"},"sidebar":"docs","previous":{"title":"G-Eval","permalink":"/docs/metrics/generation/geval"},"next":{"title":"Precision Score","permalink":"/docs/metrics/retriever/precisionscore"}}');var s=t(4848),c=t(8453);const l={id:"recallscore",title:"Recall Score"},o="Recall Score",i={},a=[{value:"<strong>Example</strong>",id:"example",level:3},{value:"<strong>Output</strong>",id:"output",level:3}];function d(e){const r={code:"code",h1:"h1",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,c.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"recall-score",children:(0,s.jsx)(r.strong,{children:"Recall Score"})})}),"\n",(0,s.jsx)(r.p,{children:"Measures how many ground truth items are retrieved."}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Inputs:"})," retrieved list and ground truth list"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Returns:"})," recall as a float between 0 and 1"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"example",children:(0,s.jsx)(r.strong,{children:"Example"})}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-py",children:"from vero import RecallScore\n\n#example inputs\n#ch_r is the retrieved citations from the retriever\n#ch_t is the ground truth citations\nch_r = [1,2,3,5,6]\nch_t = [2,3,4]\nrs = RecallScore(ch_r, ch_t)\nprint(rs.evaluate())\n"})}),"\n",(0,s.jsx)(r.h3,{id:"output",children:(0,s.jsx)(r.strong,{children:"Output"})}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-text",children:"0.76\n"})})]})}function u(e={}){const{wrapper:r}={...(0,c.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);