"use strict";(self.webpackChunkrag_eval_docs=self.webpackChunkrag_eval_docs||[]).push([[361],{8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>o});var r=n(6540);const i={},a=r.createContext(i);function s(e){const t=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(a.Provider,{value:t},e.children)}},8594:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"getting-started/quickstart","title":"QuickStart Guide","description":"Vero-Eval is an open-source evaluation framework designed to rigorously assess the performance of Retrieval-Augmented Generation (RAG) pipelines. It provides built-in tracing, logging, and a rich suite of metrics to evaluate each component in the pipeline \u2014 from retrieval and reranking to generation \u2014 all integrated end to end.","source":"@site/docs/getting-started/quickstart.md","sourceDirName":"getting-started","slug":"/getting-started/quickstart","permalink":"/docs/getting-started/quickstart","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/getting-started/quickstart.md","tags":[],"version":"current","frontMatter":{"id":"quickstart","title":"QuickStart Guide"},"sidebar":"docs","previous":{"title":"AI Evaluation Framework","permalink":"/docs/"},"next":{"title":"Project Structure","permalink":"/docs/getting-started/project-structure"}}');var i=n(4848),a=n(8453);const s={id:"quickstart",title:"QuickStart Guide"},o="Vero-Eval: A Quick Introduction",l={},c=[{value:"Starting with Vero-Eval",id:"starting-with-vero-eval",level:2},{value:"Setup",id:"setup",level:3},{value:"Example Usage",id:"example-usage",level:3}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"vero-eval-a-quick-introduction",children:(0,i.jsx)(t.strong,{children:"Vero-Eval: A Quick Introduction"})})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Vero-Eval"})," is an open-source evaluation framework designed to rigorously assess the performance of ",(0,i.jsx)(t.strong,{children:"Retrieval-Augmented Generation (RAG)"})," pipelines. It provides built-in tracing, logging, and a rich suite of metrics to evaluate each component in the pipeline \u2014 from retrieval and reranking to generation \u2014 all integrated end to end."]}),"\n",(0,i.jsx)(t.p,{children:"Key features of Vero-Eval:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Trace & Log Execution"}),": Each query runs through the RAG pipeline is logged into an SQLite database, capturing the user query, retrieved context, reranked items, and the model\u2019s output."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Component-level Metrics"}),": Evaluate intermediate pipeline stages using metrics like Precision, Recall, Sufficiency, Citation, Overlap, and Ranking metrics (e.g. MRR, MAP, NDCG)."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Generation Metrics"}),": Measure semantic, factual, and alignment quality of generated outputs using metrics such as BERTScore, ROUGE, SEMScore, AlignScore, BLEURT, and G-Eval."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Modular & Extensible"}),": Easily plug in new metric classes or custom scoring logic; the framework is designed to grow with your needs."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"End-to-End Evaluation"}),": Combine component metrics to understand the holistic performance of your RAG system \u2014 not just individual parts."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"starting-with-vero-eval",children:"Starting with Vero-Eval"}),"\n",(0,i.jsx)(t.h3,{id:"setup",children:"Setup"}),"\n",(0,i.jsx)(t.p,{children:"Install via pip (recommended inside a virtualenv):"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-bash",children:"pip install vero-eval\n"})}),"\n",(0,i.jsx)(t.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,i.jsx)(t.p,{children:"Here\u2019s how you might use it in a minimal workflow:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-py",children:'from vero.rag import SimpleRAGPipeline\nfrom vero.trace import TraceDB\nfrom vero.eval import Evaluator\n\ntrace_db = TraceDB(db_path="runs.db")\npipeline = SimpleRAGPipeline(retriever="faiss", generator="openai", trace_db=trace_db)\n\n# Run your pipeline\nrun = pipeline.run("Who invented the transistor?")\nprint("Answer:", run.answer)\n\n# Later, compute metrics for all runs\nevaluator = Evaluator(trace_db=trace_db)\nresults = evaluator.evaluate()\nprint(results)\n'})})]})}function u(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);