"use strict";(self.webpackChunkrag_eval_docs=self.webpackChunkrag_eval_docs||[]).push([[474],{2779:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"metrics/generation/bertscore","title":"BERTScore","description":"It is an automatic evaluation metric for text generation tasks that measures the similarity between candidate and reference texts using contextual embeddings from pre-trained BERT models.","source":"@site/docs/metrics/generation/bertscore.md","sourceDirName":"metrics/generation","slug":"/metrics/generation/bertscore","permalink":"/docs/metrics/generation/bertscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/generation/bertscore.md","tags":[],"version":"current","frontMatter":{"id":"bertscore","title":"BERTScore"},"sidebar":"docs","previous":{"title":"Metrics overview","permalink":"/docs/metrics/overview"},"next":{"title":"BARTScore","permalink":"/docs/metrics/generation/bartscore"}}');var s=n(4848),i=n(8453);const o={id:"bertscore",title:"BERTScore"},a="BERTScore",c={},l=[{value:"<strong>Example</strong>",id:"example",level:3},{value:"<strong>Output</strong>",id:"output",level:3}];function d(e){const t={code:"code",h1:"h1",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"bertscore",children:(0,s.jsx)(t.strong,{children:"BERTScore"})})}),"\n",(0,s.jsx)(t.p,{children:"It is an automatic evaluation metric for text generation tasks that measures the similarity between candidate and reference texts using contextual embeddings from pre-trained BERT models."}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Inputs:"})," candidate (generated) text and reference text."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Returns:"})," precision, recall, and F1-score based on token-level similarity."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"example",children:(0,s.jsx)(t.strong,{children:"Example"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-py",children:"from vero import BertScore\n\n#example inputs\n#chunks_list = [\"The cat sat on the mat.\", \"The dog barked at the mailman.\"]\n#answers_list = [\"A cat is sitting on a mat and a dog is barking at the mailman.\"]\nwith BertScore() as bs:\n    bert_results = [bs.evaluate(chunk, ans) for chunk, ans in tqdm(zip(chunks_list, answers_list), total=len(df_new))]\nbert_dicts = [{'Precision': p, 'Recall': r, 'F1score': f} for p, r, f in bert_results]\nprint(bert_dicts)\n"})}),"\n",(0,s.jsx)(t.h3,{id:"output",children:(0,s.jsx)(t.strong,{children:"Output"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"{Precision': 0.85, 'Recall': 0.80, 'F1score': 0.825}\n\n"})})]})}function u(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>a});var r=n(6540);const s={},i=r.createContext(s);function o(e){const t=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(i.Provider,{value:t},e.children)}}}]);