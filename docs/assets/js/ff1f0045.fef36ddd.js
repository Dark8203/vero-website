"use strict";(self.webpackChunkrag_eval_docs=self.webpackChunkrag_eval_docs||[]).push([[571],{5211:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>c,metadata:()=>n,toc:()=>a});const n=JSON.parse('{"id":"metrics/generation/bleurtscore","title":"BleurtScore","description":"An advanced metric based on BLEURT that produces a more nuanced weighted similarity score.","source":"@site/docs/metrics/generation/bleurtscore.md","sourceDirName":"metrics/generation","slug":"/metrics/generation/bleurtscore","permalink":"/docs/metrics/generation/bleurtscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/generation/bleurtscore.md","tags":[],"version":"current","frontMatter":{"id":"bleurtscore","title":"BleurtScore"},"sidebar":"docs","previous":{"title":"BARTScore","permalink":"/docs/metrics/generation/bartscore"},"next":{"title":"ROUGEScore","permalink":"/docs/metrics/generation/rougescore"}}');var s=r(4848),i=r(8453);const c={id:"bleurtscore",title:"BleurtScore"},l="BleurtScore (Weighted Semantic Similarity)",o={},a=[{value:"<strong>Use Cases</strong>",id:"use-cases",level:3},{value:"<strong>Insights</strong>",id:"insights",level:3},{value:"<strong>Example</strong>",id:"example",level:3},{value:"<strong>Output</strong>",id:"output",level:3}];function d(e){const t={blockquote:"blockquote",code:"code",h1:"h1",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"bleurtscore-weighted-semantic-similarity",children:(0,s.jsx)(t.strong,{children:"BleurtScore (Weighted Semantic Similarity)"})})}),"\n",(0,s.jsx)(t.p,{children:"An advanced metric based on BLEURT that produces a more nuanced weighted similarity score."}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Inputs:"})," candidate (generated) text and reference text (or query, when used as retriever metric)."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Returns:"})," a single weighted BLEURT score."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"use-cases",children:(0,s.jsx)(t.strong,{children:"Use Cases"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"As a generation metric \u2192 highlights which chunks contribute more to the output."}),"\n",(0,s.jsx)(t.li,{children:"As a retriever metric \u2192 measures semantic relationships even if exact matches are missing."}),"\n"]}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Note:"})}),"\n",(0,s.jsx)(t.p,{children:"Can be very useful for debugging:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:'If Context Recall is low, but Weighted Semantic Similarity score is high, it tells the developer: "Your retriever is finding documents that are about the right topic, but it\'s failing to find the specific sentence or fact needed for the answer"'}),"\n",(0,s.jsx)(t.li,{children:"If both scores are low, the retriever is failing at a more fundamental level"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"insights",children:(0,s.jsx)(t.strong,{children:"Insights"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"BluertScore"}),(0,s.jsx)(t.th,{children:"Inference"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"closer to 1"}),(0,s.jsx)(t.td,{children:"high semantic similarity"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"closer to 0"}),(0,s.jsx)(t.td,{children:"low semantic similarity"})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"example",children:(0,s.jsx)(t.strong,{children:"Example"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-py",children:'from vero.metrics import BleurtScore\n\n#example inputs\n#chunks_list = ["The cat sat on the mat.", "The dog barked at the mailman."]\n#answers_list = ["A cat is sitting on a mat and a dog is barking at the mailman."]\nwith BleurtScore() as bls:\n    bleurt_results = [bls.evaluate(chunk, ans) for chunk, ans in zip(chunks_list, answers_list)]\nprint(bleurt_results)\n'})}),"\n",(0,s.jsx)(t.h3,{id:"output",children:(0,s.jsx)(t.strong,{children:"Output"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"0.89\n\n"})})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,t,r)=>{r.d(t,{R:()=>c,x:()=>l});var n=r(6540);const s={},i=n.createContext(s);function c(e){const t=n.useContext(i);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),n.createElement(i.Provider,{value:t},e.children)}}}]);